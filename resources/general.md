# Mechanistic Interpretability Resources

## Table of Contents
- [Mechanistic Interpretability Resources](#mechanistic-interpretability-resources)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Tutorials](#tutorials)
  - [Historical Context](#historical-context)
  - [Survey Papers](#survey-papers)
    - [Core Mechanistic Interpretability](#core-mechanistic-interpretability)
    - [Specialized Topics](#specialized-topics)
    - [Related Alignment Surveys](#related-alignment-surveys)
  - [Video Resources](#video-resources)
  - [All-in-one](#all-in-one)

## Introduction
This is a curated collection of resources for understanding and exploring mechanistic interpretability in AI, with a focus on transformer models and large language models.

## Tutorials
- [Concrete Steps to Get Started in Transformer Mechanistic Interpretability](https://www.neelnanda.io/mechanistic-interpretability/getting-started) - Neel Nanda's blog
- [Mechanistic Interpretability Quickstart Guide](https://www.neelnanda.io/mechanistic-interpretability/getting-started) - Neel Nanda's blog
- [ARENA Mechanistic Interpretability Tutorials](https://arena-ch1-transformers.streamlit.app/) - by Callum McDougall
- [200 Concrete Open Problems in Mechanistic Interpretability](https://www.alignmentforum.org/s/yivyHaCAmMJ3CqSyj) - Introduction by Neel Nanda
- [Transformer-specific Interpretability](https://projects.illc.uva.nl/indeep/tutorial/) - EACL 2023 Tutorial

## Historical Context
- [Mechanistic?](https://arxiv.org/abs/2410.09087) - BlackBoxNLP workshop at EMNLP 2024
  > Explores multiple definitions of "mechanistic interpretability" and traces its evolution in NLP research

## Survey Papers

### Core Mechanistic Interpretability
- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613) (March 2025)
- [Representation Engineering for Large-Language Models: Survey and Research Challenges](http://arxiv.org/abs/2502.17601) (February 2025)
- [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://arxiv.org/abs/2207.13243) (SaTML 2023)
- [Neuron-level Interpretation of Deep NLP Models: A Survey](https://aclanthology.org/2022.tacl-1.74) (TACL 2022)
- [Mechanistic Interpretability for AI Safety -- A Review](http://arxiv.org/abs/2404.14082) (April 2024)
- [A Primer on the Inner Workings of Transformer-based Language Models](https://arxiv.org/abs/2405.00208) (May 2024)
- [A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646) (July 2024)

### Specialized Topics
- [Explainability for Large Language Models: A Survey](https://arxiv.org/abs/2309.01029) (TIST 2024)
- [Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability](http://arxiv.org/abs/2402.10688) (February 2024)
- [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](http://arxiv.org/abs/2403.08946) (March 2024)
- [Internal Consistency and Self-Feedback in Large Language Models: A Survey](https://arxiv.org/abs/2407.14507) (July 2024)
- [The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability](https://arxiv.org/abs/2408.01416) (August 2024)
- [Attention Heads of Large Language Models: A Survey](https://arxiv.org/abs/2409.03752) (September 2024) - [GitHub](https://github.com/IAAR-Shanghai/Awesome-Attention-Heads)

### Related Alignment Surveys
- [Large Language Model Alignment: A Survey](https://arxiv.org/abs//2309.15025) (September 2023)
- [AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852) (October 2023) - [GitHub](https://github.com/PKU-Alignment/AlignmentSurvey) | [Website](https://alignmentsurvey.com/)

## Video Resources
- [Neel Nanda's YouTube Channel](https://www.youtube.com/@neelnanda2469) - Various tutorials and explanations
- [Chris Olah - Looking Inside Neural Networks with Mechanistic Interpretability](https://www.youtube.com/watch?v=2Rdp9GvcYOE)
- [Concrete Open Problems in Mechanistic Interpretability: Neel Nanda at SERI MATS](https://www.youtube.com/watch?v=FnNTbqSG8w4)
- [BlackboxNLP's YouTube Channel](https://www.youtube.com/@blackboxnlp) - Conference presentations and workshops

## All-in-one

- **Awesome LLM Interpretability** - [github](https://github.com/cooperleong00/Awesome-LLM-Interpretability) - A curated list of interpretability resources, libraries, and research